# -*- coding: utf-8 -*-
"""MPMLP15_run.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1poWVLh83Ls8ZIePLhRpjlCs0HhXSzXd0
"""

import torch
import torch.nn as nn

import numpy as np
import argparse
import math

import cv2
import mediapipe as mp


class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.layer1 = nn.Linear(3 * 21, 1024) 
        self.layer2 = nn.Linear(1024, 1024)
        self.layer3 = nn.Linear(1024, 1024)
        self.layer4 = nn.Linear(1024, 9)

        self.activation = nn.LeakyReLU(0.1)        

    def forward(self, x):
        out = x.view(x.size(0), -1) 
        out = self.layer1(out) 
        out = self.activation(out)
        out = self.layer2(out) 
        out = self.activation(out) 
        out = self.layer3(out) 
        out = self.activation(out) 
        out = self.layer4(out)
        return out


def resizing(image, DESIRED_HEIGHT = 480, DESIRED_WIDTH = 480):
    h, w = image.shape[:2]
    if h < w:
        image = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))
    else:
        image = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))
    return image


@torch.no_grad()
def detect(player_info, weights):
    pretrained_param = weights
    class_list = ['08Paper', '04Fire', '02Light', '07Tree', '06Snake',
                  '09Devil', '03Gun', '05Scissor', '01Rock']

    # Load model
    net = MLP()
    net = torch.nn.DataParallel(net)
    device = torch.device('cpu')

    # load param
    state = torch.load(pretrained_param, map_location=device)
    net.load_state_dict(state['model_state_dict'])

    # Inference
    image = player_info['image']
    mp_hands = mp.solutions.hands
    mp_drawing = mp.solutions.drawing_utils
    mp_drawing_styles = mp.solutions.drawing_styles
    with mp_hands.Hands(
        static_image_mode=True,
        max_num_hands=1,
        min_detection_confidence=0.7) as hands:

        # Convert the BGR image to RGB, flip the image around y-axis for correct
        # handedness output and process it with MediaPipe Hands.
        results = hands.process(cv2.flip(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), 1))

        # Print handedness (left v.s. right hand).
        if not results.multi_hand_landmarks:
            result = None
            resized_image = None
        else:
            image_height, image_width, _ = image.shape
            annotated_image = cv2.flip(image.copy(), 1)
            for hand_landmarks in results.multi_hand_landmarks:
                mp_drawing.draw_landmarks(
                    annotated_image,
                    hand_landmarks,
                    mp_hands.HAND_CONNECTIONS,
                    mp_drawing_styles.get_default_hand_landmarks_style(),
                    mp_drawing_styles.get_default_hand_connections_style()
                )
            landmark_data = np.zeros((21, 3))
            for landmark_idx, normalized_landmark in enumerate(hand_landmarks.landmark):
                landmark_data[landmark_idx, :] = np.array([[normalized_landmark.x, normalized_landmark.y, normalized_landmark.z]])

            # MLP inference
            landmark_torch = torch.tensor(landmark_data).float()
            landmark_torch = landmark_torch.unsqueeze(0)
            net_res = net(landmark_torch)
            res_idx = torch.argmax(net_res)
            result = class_list[res_idx]
            resized_image = resizing(cv2.flip(annotated_image, 1))

    player_info['class'] = result
    player_info['predictions'] = resized_image
    return player_info


if __name__ == '__main__':
    pass
